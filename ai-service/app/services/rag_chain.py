import google.generativeai as genai
from typing import List, Dict
from app.services.document_loader import DocumentLoader
from app.services.embeddings import EmbeddingService
from app.services.vector_store import FAISSVectorStore
from app.config.settings import settings
from app.config.constants import SYSTEM_PROMPT_TEMPLATE


class RAGService:
    def __init__(self):
        self.llm = None
        self.embeddings = None
        self.vector_store = None
        self.doc_loader = DocumentLoader(settings.data_path)

    async def initialize(self):
        print("=" * 60)
        print("ğŸš€ RAG Service áƒ˜áƒœáƒ˜áƒªáƒ˜áƒáƒšáƒ˜áƒ–áƒáƒªáƒ˜áƒ...")
        print("=" * 60)

        genai.configure(api_key=settings.google_api_key)
        self.llm = genai.GenerativeModel(settings.llm_model)
        print(f"âœ… Gemini: {settings.llm_model}")

        self.embeddings = EmbeddingService(settings.embedding_model)

        self.vector_store = FAISSVectorStore(
            dimension=self.embeddings.get_dimension(),
            index_path=settings.index_path
        )

        if self.vector_store.count == 0:
            await self._load_documents()
        else:
            print(f"âœ… Vector store áƒ£áƒ™áƒ•áƒ” áƒ¨áƒ”áƒ•áƒ¡áƒ”áƒ‘áƒ£áƒšáƒ˜áƒ: {self.vector_store.count} áƒ“áƒáƒ™áƒ£áƒ›áƒ”áƒœáƒ¢áƒ˜")

        print("=" * 60)
        print("ğŸ‰ RAG Service áƒ›áƒ–áƒáƒ“áƒáƒ!")
        print("=" * 60)

    async def _load_documents(self):
        print("ğŸ“„ ilaria-instructions.md áƒ˜áƒ¢áƒ•áƒ˜áƒ áƒ—áƒ”áƒ‘áƒ...")

        try:
            chunks = self.doc_loader.load_and_split(
                chunk_size=settings.chunk_size,
                chunk_overlap=settings.chunk_overlap
            )

            texts = [chunk["content"] for chunk in chunks]
            metadatas = [chunk["metadata"] for chunk in chunks]

            print("ğŸ”„ Embeddings áƒ˜áƒ¥áƒ›áƒœáƒ”áƒ‘áƒ...")
            embeddings = self.embeddings.embed_documents(texts)

            self.vector_store.add_documents(texts, embeddings, metadatas)
            print(f"âœ… áƒ“áƒáƒ˜áƒœáƒ“áƒ”áƒ¥áƒ¡áƒ˜áƒ áƒ“áƒ {len(chunks)} chunk")

        except FileNotFoundError as e:
            print(f"âŒ áƒ¨áƒ”áƒªáƒ“áƒáƒ›áƒ: {e}")
            raise

    async def generate_response(
        self, query: str, history: List[Dict] = None
    ) -> Dict:
        if not query.strip():
            raise ValueError("Query áƒáƒ  áƒ¨áƒ”áƒ˜áƒ«áƒšáƒ”áƒ‘áƒ áƒ˜áƒ§áƒáƒ¡ áƒªáƒáƒ áƒ˜áƒ”áƒšáƒ˜")

        try:
            query_embedding = self.embeddings.embed_query(query)

            similar_docs = self.vector_store.similarity_search(
                query_embedding, k=settings.top_k_results
            )

            if not similar_docs:
                return {
                    "answer": "áƒ£áƒ™áƒáƒªáƒ áƒáƒ•áƒáƒ“, áƒ•áƒ”áƒ  áƒ•áƒ˜áƒáƒáƒ•áƒ” áƒ áƒ”áƒšáƒ”áƒ•áƒáƒœáƒ¢áƒ£áƒ áƒ˜ áƒ˜áƒœáƒ¤áƒáƒ áƒ›áƒáƒªáƒ˜áƒ.",
                    "sources": [],
                    "metadata": {"retrieved_docs": 0}
                }

            context = "\n\n".join([
                f"[áƒ“áƒáƒ™áƒ£áƒ›áƒ”áƒœáƒ¢áƒ˜ {i+1}]:\n{doc['content']}"
                for i, doc in enumerate(similar_docs)
            ])

            history_text = ""
            if history:
                history_text = "\n".join([
                    f"{'áƒ›áƒáƒ›áƒ®áƒ›áƒáƒ áƒ”áƒ‘áƒ”áƒšáƒ˜' if h.get('role') == 'user' else 'áƒáƒ¡áƒ˜áƒ¡áƒ¢áƒ”áƒœáƒ¢áƒ˜'}: {h.get('content', '')}"
                    for h in history[-5:]
                ])

            prompt = SYSTEM_PROMPT_TEMPLATE.format(
                context=context,
                history=history_text if history_text else "áƒáƒ  áƒáƒ áƒ˜áƒ¡",
                query=query
            )

            response = self.llm.generate_content(
                prompt,
                generation_config=genai.types.GenerationConfig(
                    temperature=settings.temperature,
                    max_output_tokens=settings.max_tokens,
                )
            )

            answer = response.text.strip()

            sources = [
                {
                    "id": i + 1,
                    "content": doc["content"][:200] + "...",
                    "section": doc["metadata"].get("section", "Unknown")
                }
                for i, doc in enumerate(similar_docs[:3])
            ]

            return {
                "answer": answer,
                "sources": sources,
                "metadata": {
                    "model": settings.llm_model,
                    "retrieved_docs": len(similar_docs)
                }
            }

        except Exception as e:
            print(f"âŒ áƒ¨áƒ”áƒªáƒ“áƒáƒ›áƒ: {str(e)}")
            raise
